# 1. Introduction

### 연구 배경 및 문제 정의

팀 내부 Notion 및 Github 문서를 지식 베이스로 활용하는 RAG 기반 Slack Bot 프로젝트를 개발하면서, 생성된 답변 품질이 낮은 이슈가 있었습니다. 특히 검색된 문서의 수와 문서의 분할 방식에 따라 생성되는 답변 품질이 유의미하게 차이나는 이슈를 확인했습니다.

### 실험 목표

본 실험은 TopK, Chunk Size, Overlap 파라미터가 생성된 응답의 품질과 시스템에 미치는 영향을 정량적으로 파악하고, 각 파라미터 사이의 trade-off 를 파악하여 최적의 TopK, Chunk Size, Overlap 값을 찾아내는 것을 목표로 진행했습니다.

# 2. RAG 소개

### RAG란?

RAG란 LLM 이 답변을 생성할 때 외부 지식 베이스에서 관련 있는 정보를 검색하고 (Retrieval) 검색된 정보를 바탕으로 답변을 생성 (Generation) 하는 기술입니다.

RAG는 외부 지식 베이스 기반으로 답변을 생성하기 때문에 답변의 근거를 제시할 수 있어 LLM 의 고질적인 환각 문제를 줄일 수 있습니다.
또한 외부 지식 베이스를 활용하여 LLM이 학습하지 못한 최신 정보나 특정 분야의 도메인 지식에 대한 답변이 필요한 경우 유용하게 활용할 수 있습니다.
RAG는 LLM 이 학습 할 수 없는 기업 내부의 프라이빗한 데이터 기반의 답변 생성이 필요할때도 활용됩니다.

### RAG의 주요 구성 요소 및 프로세스

RAG 의 프로세스는 크게 아래의 3가지로 나눌 수 있습니다.

1. 청킹 및 인덱싱

- Chunking: 방대한량의 문서를 모델이 처리 가능한 적절한 크기로 자릅니다.
- Embedding: 자른 문서를 벡터화하여 의미론적 공간에 매핑합니다.(gemini-embedding-001, text-embedding-3-small etc..)
- Vertor Database: 임베딩된 벡터를 저장합니다. (Pinecone, ChromaDB etc..)

2. 검색

- Query Embedding: 사용자의 질문을 문서 임베딩에 사용한 모델로 임베딩합니다.
- Retrieval: Vector 데이터 베이스에서 사용자의 질문과 유사도가 높은 k 개의 문서 청크를 검색합니다.

3. 증강 및 생성

- Augmentation: 검색된 k 개의 문서 청크를 프롬프트에 넣어 LLM 에 전달합니다.
- Generation: LLM 은 사용자 질문과 유사한 k 개의 문서 청크를 기반으로 답변을 생성합니다.


### RAG 성능에 영향을 미치는 요소

RAG 성능에 영향을 미치는 요소로는 데이터 전처리 및 인덱싱, 임베딩 모델, 검색 알고리즘, 검색 결과 최적화, 프롬프트 엔지니어링이 있습니다.

- 데이터 전처리 및 인덱싱: 문서를 모델이 이해하기 좋은 형태로 변형합니다. (Chunking, Overlap, Metadata)
- 임베딩 모델: 텍스트를 의미론적 벡터로 변환합니다. (gemini-embedding-001, text-embedding-3-small) 
- 검색 알고리즘: 좋은 검색 결과를 얻기 위한 알고리즘입니다. (Hybrid Search, Query Expantion, Query Rewriting)
- 검색 결과 최적화: 검색 결과 중 LLM 의 한정된 Context 안에 무엇을 넣어줄지에 대한 알고리즘입니다. (Re-Ranking, Context Compression)
- 프롬프트 엔지니어링: 검색된 결과를 프롬프트로 구성하는 기법입니다. (Instruction Tuning, Few-shot Prompting)


본 실험에서는 데이터 전처리 과정에 해당되는 Chunk Size, Overlap과 몇개의 문서 청크를 검색할지에 대한 TopK에 집중하여 실험을 진행하였습니다.

- Chunk Size: 문서를 나누는 청크의 크기 (256, 512, 1024 characters)
- Overlap: 인접한 청크 간 중복되는 문자의 비율 (0%, 15%, 30%)
- TopK: 벡터 검색 시 반환할 유사 문서의 개수 (3, 5, 10개) 

# 3. System Overview

실험을 본격적으로 소개하기 전에 Slack Rag Bot 프로젝트에 대해서 간략하게 소개하겠습니다.

### Project Overview

많은 개발팀에서는 Github 에 저장된 코드와 기술 문서, Notion 에 저장된 업무 프로세스와 프로젝트 요구사항 등의 문서가 여러곳에 분산되어 있어, 필요한 정보를 찾는데 많은 시간이 소요되는 문제를 겪고 있습니다.
Slack Rag 봇 프로젝트는 이러한 문제를 해결하기 위해 RAG 기술을 활용하여 자연어 질문만으로 개발 문서와 코드베이스를 효율적으로 검색하고 답변 받을 수 있는 시스템을 구축했습니다.
시스템은 개발팀이 가장 많이 사용하고 있는 Slack 생태계와 통합하여 업무 프로세스에서 효율적을 활용 가능하도록 지원하고 있습니다.

### Knowledge Base Ingestion Pipeline

RAG 시스템의 핵심적인 지식 베이스 파이프라인은 아래의 단계로 구성되어 있습니다.
아래의 파이프라인은 Notion 과 Github 연동 시점에 진행되며, 지식 베이스의 최신화를 위해 Cron Job 을 통해 주기적으로 실행됩니다.

1. 데이터 수집
  - Github: Octokit API 를 통해 접근 가능한 모든 리포지토리에서 파일을 수집합니다.
  - Notion: Notion API 를 통해 지정된 데이터 베이스에서 모든 파일을 수집합니다.

2. 텍스트 청킹: 수집된 파일은 LLM 이 처리 가능한 적절한 크기로 분할합니다.
  - Chunk Size: 1000 characters
  - Overlap: 200 characters

3. 임베딩 벡터 생성: 청킹된 문서를 gemini-embedding-001 를 통해 임베딩 벡터를 생성합니다.
4. 임베딩 벡터 저장: 임베딩 벡터를 Pinecone 벡터 스토어에 저장합니다.

`2. 텍스트 청킹` 단계의 파라미터가 이 실험을 통해 최적화하고자 하는 핵심 변수입니다. 
Chunk Size 와 Overlap 이 검색 품질과 답변 생성에 어떤 영향을 미치는지 정량적으로 분석하는 것이 실험의 주요 목표입니다.

### Rag Pipeline Architecture

사용자가 Slack 에서 봇에게 질문을 하면 다음의 프로세스로 답변이 생성됩니다.

1. 질문 분류: LangGraph 를 이용하여 사용자의 질문을 3가지 유형으로 자동 분류합니다.
  - Github: 코드 API 문서, 기술 관련 질문
  - Notion: 업무 프로세스, 가이드라인, 정책 관련 질문
  - Conversation: 일반 대화 또는 Slack 대화 내용 관련 질문

2. 벡터 스토어 검색
  - 사용자의 질문을 임베딩 하여 적절한 지식 베이스의 벡터 스토어에 검색합니다. (Notion 혹은 Gihub)
  - Pinecone 의 유사도 검색으로 가장 유사도 점수가 높은 상위 K 개의 문서 청크를 가져옵니다.

3. 증강 및 생성
  - 검색된 문서 청크로 프롬프트를 구성하여 Gemini 모델을 통해 답변을 생성합니다.
  - Temperature 0.7로 설정하여 창의성과 일관성의 균형을 적절하게 맞춥니다.

### Multi-tenant Architecture

본 시스템은 많은 개발팀이 사용 가능하도록 확장 가능한 멀티 테넌트 아키텍쳐로 설계했습니다. 
각 개발팀의 데이터를 안전하게 분리하기 위해 Pinecone 네임스페이스로 각 테넌트들의 데이터를 분리하였으며, 각 요청은 테넌트 별로 서로 다른 서브 트리에 의해 처리됩니다.
요청을 처리하는 서브트리는 노션 서비스 클래스, 깃 허브 서비스 클래스, Pinecone 클라이언트 클래스 등으로 구성되어있습니다.
또한 NestJS의 서브트리 캐싱을 통해 같은 테넌트의 요청들은 캐싱된 하나의 서브트리를 사용하도록하여 불필요한 시스템 리스소 사용을 방지하고 있습니다.

- Pinecone
  - namespace 별로 데이터를 분리하여 테넌트 별로 데이터가 안전하게 분리되도록하였습니다.
- 서브트리 캐싱
  - Request Body 의 포함된 `teamId` 기반으로 테넌트 별로 별도의 서브트리를 사용합니다.
  - 각 테넌트는 캐싱된 동일한 서브트리를 사용하여 불피요한 시스템 오버해드를 방지합니다.


# 4. Experimental Setup

RAG 파이프라인의 성능 최적화 실험을 위한 데이터셋 구성, 파라미터 구성, 사용 모델에 대해 설명합니다.


## Dataset & Embedding
 
### 테스트 데이터

내부 깃허브 및 노션에 있는 데이터량에 제한이 있어서, 쿠버네티스 한국어 문서를 실험에 사용하였습니다. 
쿠버네티스 한국어 문서는 개발 팀 노션 문서의 성격과 유사도가 높다고 판단되어서 쿠버네티스 문서를 사용해 실험을 진행했습니다.


### 임베딩 및 벡터스토어

RAG 기술 기반 슬랙 봇 프로젝트에서 사용하고 있는 구글의 gemini-embedding-001 임베딩 모델을 사용했습니다. 
임베딩 벡터의 차원은 일반적으로 사용되는 1024로 설정하고 실험을 진행했습니다.
임베딩 벡터는 Pinecone 벡터 스토어에 저장했습니다. 실험 케이스별 서로 다른 네임스페이스를 사용하도록 실험을 설계했습니다.
각 네임스페이스 별로 저장되어 있는 데이터 구성은 다음과 같습니다.

- cs256-ov0: Chunk Size 256, Overlap 0%
- cs512-ov0: Chunk Size 512, Overlap 0%
- cs1024-ov0: Chunk Size 1024, Overlap 0%
- cs256-ov15: Chunk Size 256, Overlap 15%
- cs512-ov15: Chunk Size 512, Overlap 15%
- cs1024-ov15: Chunk Size 1024, Overlap 15%
- cs256-ov30: Chunk Size 256, Overlap 30%
- cs512-ov30: Chunk Size 512, Overlap 30%
- cs1024-ov30: Chunk Size 1024, Overlap 30%


## LLM Model & Prompt

### LLM Model

RAG 기술 기반 슬랙 봇 프로젝트에서 사용하고 있는 구글의 gemini-3-flash-preview 모델을 사용하여 높은 추론 속도와 이해도를 확보하였습니다.

### Prompt

"제공된 컨텍스트 내에서만 답변하고, 정보가 부족할 경우 모른다고 답변 할 것" 을 명시하여 할루시네이션 발생 가능성을 통제하고 지식 증강 성능에 집중하였습니다.

## Experimental Variables

본 실험에서는 RAG 시스템의 성능에 영향을 미치는 3가지 핵심 파라미터(TopK, Chunk Size, Overlap)를 실험 변수로 설정하였습니다.

### TopK

벡터 검색 시 반환할 문서 청크의 개수를 의미합니다. 
TopK 값이 클수록 더 많은 컨텍스트를 LLM에 제공하지만, 관련성이 낮은 문서가 포함될 가능성이 높아지고 토큰 비용이 증가합니다.

- 실험 값: 1, 3, 5
- 선택 이유: 적은 값, 중간 값, 높은 값을 비교해서 품질과 비용간의 trade-off 를 분석하고자 합니다.

### Chunk Size

문서를 분할하는 청크의 크기를 의미합니다. 
Chunk Size 가 작으면 정밀한 검색이 가능하지만 문맥이 단절될 수 있고, 크면 더 많은 문맥을 포함하지만 검색 정확도가 떨어질 수 있습니다.

- 실험 값: 256, 512, 1024 characters
- 선택 이유: 작은 청크 크기, 중간 청크 크기, 대형 청크 크기를 비교하여 청크 크기가 검색 및 답변 품질에 미치는 영향을 분석하고자 합니다.

### Overlap

인접한 청크 간 중복되는 문자의 비율을 의미합니다. 
Overlap이 높으면 문맥 보존성이 향상되지만 저장 공간과 검색 시간이 증가합니다.

- 실험 값: 0%, 15%, 30%
- 선택 이유: 중복 없음부터 적은 중복, 높은 중복까지 설정하여 문맥 보존과 효율성 간의 균형점을 분석하고자 합니다.

# 5. Evaluation Metrics

RAGAS 기반 응답 품질 지표와 시스템 성능 지표를 함께 측정하여, 단순 정확도 비교뿐만아니라 정확도와 성능을 trade-off 관점에서 분석하여 최적의 파라미터를 도출하고자합니다.

### 응답 품질 지표

RAG 시스템의 응답 품질을 정량적으로 평가하기 위해 RAGAS 프레임워크를 활용하였습니다.
RAGAS는 LLM-as-a-Judge 방식을 기반으로 평가를 자동화할 수 있고, 검색 단계와 생성 단계의 품질을 분리하여 평가할 수 있다는 장점이 있습니다.

- Faithfulness
  - 생성된 응답이 검색된 컨텍스트에 기반하여 생성되었는지를 평가하는 지표입니다.
  - 지표를 통해 LLM 이 제공된 문서를 벗어나 임의의 답변을 생성하는 환각 현상 발생 여부를 확인할 수 있습니다.
- Answer Relevancy
  - 생성된 응답이 사용자의 질문에 얼마나 적절하게 응답하고 있는지를 평가하는 지표입니다.
  - 검색을 통해 문서가 충분히 제공되었더라도, 질문의 의도와 무관한 답변이 생성될 수 있기 때문에 이 지표를 통해 응답의 질을 최종적으로 판단하였습니다.


### 검색 품질 지표

RAG 시스템에 답변 생성 품질은 검색 품질에 크게 의존하기 때문에, 검색 단계의 성능을 평가하였습니다.
측정한 지표는 아래과 같습니다.

- Context Recall
  - 응답을 생성하는 데 필요한 핵심 정보가 얼마나 검색되었는지를 평가하는 지표입니다.
  - TopK 나 Chunk Size 증가에 검색 범위가 달라질때 누락이 얼마나 발생하는지 평가합니다.
- Context Precision
  - 검색된 청크 중 실제로 응답 생성에 유의미한 청크의 비율을 평가하는 지표입니다.
  - TopK 나 Overlap 증가로 인해 불필요한 노이이즈 데이터가 얼마나 발생하는지 평가합니다.

  
### 시스템 성능 지표

응답 품질 향상이 시스템 성능 저하를 동반할 수 있기 때문에 시스템 성능 지표를 함께 측정하였습니다.
아래의 Latency 지표와 Token Usage 지표를 측정하였습니다.

- Latency: Overlap 및 Chunk Size 증가에 따른 지연 시간 증가를 분석하기 위해 측정하였습니다.
  - TTFT: 질의 요청 이후 첫 토큰이 생성되기까지의 시간까지의 시간입니다.
  - End-to-End Latency: 전체 응답이 생성되기까지의 총 소요 시간입니다.
- Token Usage: Overlap 및 Chunk Size 증가에 따른 비용 증가를 분석하기 위해 측정하였습니다.
  - Prompt Tokens
  - Completion Tokens
  - Total Tokens

# 6. Hypotheses

실험 진행전 RAG 시스템의 핵심 파라미터인 Top-K, Chunk Size, Overlap이 응답 품질 및 시스템 성능에 미치는 영향을 분석하기 위해 다음과 같은 가설을 설정하였습니다.

## TopK 증가에 따른 품질 변화

### Answer Relevancy

TopK 가 증가하면 더 많은 문서의 청크를 컨텍스트에 포함하므로 Answer Relevancy 지표는 증가할 것으로 예상됩니다.
TopK 가 일정 값 이상에 도달한 이후부터는 컨텍스트에 질문과 유사도가 낮은 문서 청크가 포함되어 Answer Relevancy 가 유지되거나 저하되는 현상이 발생 할 것으로 예상됩니다.
TopK 의 증가에 따라 Answer Relevancy 증가가 멈추게 되는 지점을 찾는게 실험의 주요 목적입니다.

### Faithfulness

TopK 변동에 따라 컨텍스트 크기가 변동 되어도 Faithfulness 지표는 변동이 없을 것으로 예상됩니다.
실험에 사용되는 gemini-3-flash-preview 모델은 강력한 추론 성능을 보유하고 있으므로, TopK 변화에 따른 Faithfulness 지표의 유의미한 지표 차이는 없을 것으로 예상하였습니다.

### Context Recall

TopK 가 증가함에 따라 검색 단게에서 더 많은 문서 청크가 포함되므로 Context Recall 은 TopK 에 비례하여 증가할 것으로 예상됩니다.
그러나 TopK 가 일정 수준을 초과할 경우, 정답 생성에 필요 없는 문서 청크가 검색 결과에 추가되면서 Context Recall 은 더 이상 증가하지 않고 유지되는 경향성을 보일 것으로 예측됩니다.

### Context Precision

TopK 가 특정값 이하인 구간에서는 TopK 가 증가하더라도 질문과 유사도 점수가 높은 문서가 주로 검색되어 Context Precision 이 높은 수준으로 유지 될 것으로 예상됩니다.
반면 TopK 가 특정 임계값을 초과할 경우 질문과 유사도 점수가 낮은 문서 청크가 검색 결과에 포함되면서 Context Precision 은 TopK 증가에 따라 감소하는 경향을 보일 것으로 예상됩니다.

### Latency & Token Usage

TopK 가 증가함에 따라 검색된 문서 청크 수가 증가합니다. 이에 따라 LLM 모델에 전달되는 컨텍스트 크기가 비례하며 증가할 것으로 예상됩니다.
따라서 TopK 증가에 따라 Latency 와 Token Usage 모두 증가하는 경향성을 보일 것으로 예상됩니다.

## Chunk Size 증가에 따른 품질 변화

### Answer Relevancy

Chunk Size 가 작은 경우 문서가 세밀하게 분할되어 각 청크의 임베딩이 문서내 개념이나 키워드를 효과적으로 반영할 수 있을 것으로 예상됩니다.
이에 따라 질문에 유사한 청크를 검색하기 유리할 것으로 판단됩니다.
다만 각 청크에 포함된 정보량이 적어 응답 생성에 필요한 충분한 문맥이 모델에게 제공되지 않아 Answer Relevancy 가 낮을 것으로 예상됩니다.

Chunk Size 를 적절한 수준으로 설정한 경우 문서가 개념이나 키워드가 임베딩 벡터에 효과적으로 반영되면서도, 하나의 청크에 응답 생성에 필요한 정보가 충분하게 포함될 것으로 예상됩니다. 
이에 따라 질문에 연관성이 높은 문서 청크가 검색되고 청크 내부에 충분한 정보가 포함되어  Answer Relevancy 가 높을 것으로 예상됩니다.

Chunk Size 가 큰 경우 청크 내부에 포함되는 정보가 과도하게 많아져 핵심 개념이나 키워드가 임베딩 벡터에 충분히 반영되지 못할 가능성이 있습니다.
이로인해 질문에 대한 연관성이 높은 문서 청크를 적절하게 검색하지 못하여 Answer Relevancy 가 낮을 것으로 예상됩니다.

적절한 중간 사이즈의 청크 크기를 찾는것에 집중햐여 실험을 진행하고자 함니다.

### Faithfulness

gemini-3-flash-preview 모델은 비교적 강력한 추론 성능을 보유하고 있으므로 Chunk Size 에 변화에 따라 컨텍스트 크기가 변동되더라도 Faithfulness 는 큰 변동 없이 유지될 것으로 예상됩니다.
Faithfulness 지표는 컨텍스트의 크기 보다 모델의 성능에 영향을 더 많이 받을 것으로 예상됩니다.

### Context Recall

Chunk Size 가 작은 경우 문서가 세밀하게 분할되어 임베딩 벡터에 청크의 핵심 키워드나 개념이 효과적으로 포함될 것입니다.
이에 따라 질문과 연관성이 높은 청크가 검색될 가능성이 높지만, 정답 생성에 필요한 충분한 정보가 청크에 포함되지 않아 Context Recall 이 낮을 것으로 예상됩니다.

Chunk Size 를 적절한 수준의 중간 사이즈로 설정한 경우 임베딩 벡터에 청크의 핵심 키워드나 개념이 효과적으로 포함되어 검색에 유리하고, 
청크 내부에 답변 생성에 필요한 충분한 양의 정보가 포함되어 Context Recall 이 높을 것으로 예상됩니다.

Chunk Size 가 큰 경우 임베딩 벡터가 청크의 핵심 키워드나 개념을 효과적으로 포함하지 못하여
답변 생성에 필요한 청크가 제대로 검색되지 않을 확률이 높습니다. 따라서 Context Recall 이 낮을 것으로 예상됩니다.

### Context Precision

Chunk Size 가 작은 경우 문서가 세밀하게 분할되어 임베딩 벡터에 청크의 핵심 키워드나 개념이 효과적으로 포함될 것입니다.
하지만 문서가 세밀하게 분할되어 있기 때문에 각 청크는 유용한 답변을 생성하기 위한 충분한 양의 정보가 포함되어있지 않을 확률이 높습니다.
따라서 Chunk Size 가 작은 경우 Context Precision 이 작을 것으로 예상됩니다.

Chunk Size 가 중간 수준으로 설정한 경우에도 임베딩 벡터에 청크의 핵심 키워드나 개념이 효과적으로 포함 될 것입니다.
또한 Chunk Size 가 작은 경우에 비해 청크 내부에 답변 생성을 위한 충분한 정보가 포함될 가능성이 높습니다.
따라서 Chunk Size 가 보통 크기인 경우 Context Precision 는 높은 값을 보일 것으로 예상됩니다.

Chunk Size 가 큰 경우에는 하나의 청크에 포함되는 정보가 과도하게 많아져 임베딩 벡터에 청크의 핵심 개념과 키워드를 포함하지 못할 것 입니다.
따라서 질문과 관련된 청크를 검색하지 못할 가능성이 높습니다. 따라서 Chunk Size 가 큰 경우 Context Precision 은 낮은 값을 보일 것으로 예상되니다.

### Latency & Token Usage

Chunk Size 가 증가함에 따라 청크 길이가 길어져 LLM 모델에 입력되는 입력 되는 컨텍스트가 커집니다.
따라서 Chunk Size 가 증가하면 Latency 와 Token Usage 는 전반적으로 증가하는 경향성을 보일 것으로 예상됩니다.

## Overlap 증가에 따른 품질 변화

### Answer Relevancy

Overlap 이 증가하면 인접한 문서 청크 간에 중복이 발생하여 문맥이 단절 되는 문제가 완화되고 답변 생성에 필요한 정보가 연속적으로 컨텍스트에 포함될 가능성이 높아질 것으로 예상됩니다.

Chunk Size 가 작은 경우에는 Overlap 증가에 따라 답변에 필요한 정보가 단절되지 않고 이어서 제공될 것입니다.
따라서 Answer Relevancy 가 유의미하게 증가할 것으로 예상됩니다. 

하지만 Chunk Size 가 큰 경우에는 Overlap 을 증가시켜도 문서 청크 내부에 이미 답변에 필요한 충분한 량의 정보가 들어 있으므로 유의미한 Answer Relevancy 증가는 없을 것으로 예상됩니다.

### Faithfulness

gemini-3-flash-preview 모델은 비교적 강력한 추론 성능을 보유하고 있으므로 Chunk Size 에 변화에 따라 컨텍스트 크기가 변동되더라도 Faithfulness 는 큰 변동 없이 유지될 것으로 예상됩니다.
Faithfulness 지표는 컨텍스트의 크기 보다 모델의 성능에 영향을 더 많이 받을 것으로 예상됩니다.

### Context Recall

Chunk Size가 작은 경우 Overlap이 커질 수록 Chunk Size로 인해 답변에 필요한 정보가 단절되는 현상이 완화되고 연속된 컨텍스트가 연속해서 제공될 가능성이 커지므로,
모범 답변 생성에 필요한 개념이 검색 결과에 포함될 가능성이 커집니다. 따라서 Context Recall 이 증가할 것으로 예상됩니다.

Chunk Size가 큰 경우 Overlap 증가와 관계 없이 이미 하나의 문서 청크 내부에 충분한 정보가 들어있기 때문에 유의미한 Conext Recall 증가는 없을 것으로 예상됩니다.

### Context Precision

Overlap이 증가함에 따라 동일한 정보가 여러 청크에 중복되어 포함될 가능성이 커집니다. 따라서 질문에 관련 있는 핵심 정보가 여러 청크에 반복적으로 포함될 수 있습니다.
그러므로 핵심 정보를 포함한 유사한 청크들이 검색 결과 상위 랭크에 배치되어 Context Precision이 증가할 것으로 예상됩니다.

### Latency & Token Usage

Top-K, Chunk Size 가 동일하다고 가정할 경우, Overlap 이 변동되더라도 문서 각 청크의 길이는 유지되므로 LLM 모델의 Token Usage 는 동일할 것입니다.
다만 Overlap 이 증가함에 따라 벡터 스토어에 증가되는 벡터의 수가 증가하므로 검색 단계에서 처리해야하는 벡터 수가 증가합니다.
따라서 Latency 가 소폭으로 증가할 가능성이 있을 것으로 예상됩니다.

# 7. Result

## TopK

![TopK vs Quality](./plot/topk-vs-quality.png)

Faithfulness는 topK가 1일때 0.95, topK가 3일때 0.92, topK가 5 일때 0.93 값을 보였습니다.
Faithfulness는 모든 topK 에서 전반적으로 높은 값을 보였습니다.

Aanswer Relevency는 topK가 1일때 0.61, topK가 3일때 0.6916, topK가 5일때 0.61 값을 보였습니다.
Aanswer Relevency는 topK가 3일때 최대값을 보였습니다.

Context Recall은 topK 가 1일때 0.71, topK가 3일때 0.87, topK가 5일때 0.88 값을 보였습니다.
Context Recall은 topK가 증가에 따라 같이 증가하는 경향성을 보였습니다.

Context Precision은 topK 가 1일때 0.66, topK가 3일때 0.82, topK가 5일때 0.73 값을 보였습니다.
Context Precision은 topK가 3일때 최대값을 보였습니다.


![TopK vs Cost](./plot/topk-vs-cost.png)

latency는 topK가 1일때 5.53초, topK가 3일때 5.96초, topK가 5일때 6.72초 값을 보였습니다.
latency는 topK가 가 증가하면서 약간씩 증가하는 늘어나는 경향을 보였습니다.

Total Tokens은 topK가 1일때 894, topK가 3일때 1364, topK가 5일때 1912 값을 보였습니다.
Total Tokens은 topK가 증가함에따라 비례하며 증가하는 경향성을 보였습니다.

## Chunk Size

![Chunk Size vs Quality](./plot/chunk-size-vs-quality.png)

Faithfulness는 Chunk Size가 256일때 0.84, Chunk Size가 512일때 0.92, Chunk Size가 1024일때 0.96 값을 보였습니다.

Aanswer Relevency는 Chunk Size가 256일때 0.63, Chunk Size가 512일때 0.69, Chunk Size가 1024일때 0.57 값을 보였습니다.
Aanswer Relevency는 Chunk Size가 512일때 최대값을 보였습니다.

Context Recall은 Chunk Size 가 256일때 0.66, Chunk Size가 512일때 0.87, Chunk Size가 1024일때 0.90 값을 보였습니다.
Context Recall은 Chunk Size가 증가에 따라 같이 증가하는 경향성을 보였습니다.

Context Precision은 Chunk Size 가 256일때 0.49, Chunk Size가 512일때 0.82, Chunk Size가 1024일때 0.90 값을 보였습니다.
Context Precision은 Chunk Size가 1024일때 최대값을 보였습니다.



![Chunk Size vs Cost](./plot/chunk-size-vs-cost.png)

latency는 Chunk Size가 256일때 7.20초, Chunk Size가 512일때 5.96초, Chunk Size가 1024일때 6.95초 값을 보였습니다.
latency는 Chunk Size가 256일때 최대값을 보였고, Chunk Size가 512일때 최솟값을 보였습니다.

Total Tokens은 Chunk Size가 256일때 1263, Chunk Size가 512일때 1364, Chunk Size가 1024일때 2142 값을 보였습니다.
Total Tokens은 Chunk Size가 증가함에 따라 증가하는 경향성을 보였습니다.

## Overlap

![Overlap vs Quality](./plot/overlap-vs-quality.png)

Faithfulness는 Overlap이 0%일떄 0.89, Overlap이 15%일때 0.92, Overlap이 30%일때 0.90 값을 보였습니다.

Aanswer Relevency는 Overlap이 0%일떄 0.64, Overlap이 15%일때 0.69, Overlap이 30%일때 0.64 값을 보였습니다.
Aanswer Relevency는 Overlap이 15%일때 최대값을 보였습니다.

Context Recall은 Chunk Size 가 0%일떄 0.87, Overlap이 15%일때 0.87, Overlap이 30%일때 0.87 값을 보였습니다.
Context Recall은 Overlap값과 관계 없이 일관된 수치를 보였습니다.

Context Precision은 Chunk Size 가 0%일떄 0.76, Overlap이 15%일때 0.82, Overlap이 30%일때 0.77 값을 보였습니다.
Context Precision은 Overlap이 15%일때 최댓값을 보였습니다.

![Overlap vs Cost](./plot/overlap-vs-cost.png)

latency는 Overlap이 0%일떄 6.66초, Overlap이 15%일때 5.96초, Overlap이 30%일때 6.43초 값을 보였습니다.
latency는 Overlap이 0%일떄 최대값을 보였고, Overlap이 15%일때 최솟값을 보였습니다.

Total Tokens은 Overlap이 0%일떄 1491, Overlap이 15%일때 1364, Overlap이 30%일때 1477 값을 보였습니다.
Total Tokens은 0%일떄 최대값을 보였고, Overlap이 15%일때 최솟값을 보였습니다.


# 8. Discussion
